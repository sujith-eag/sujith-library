# Analysis Framework

### What is meant by the "input size" of an algorithm? How is it typically measured for problems like sorting, matrix multiplication, or graph traversal?

The **input size** of an algorithm is a parameter that indicates how large the algorithm’s input is. Most algorithms take longer to run on larger inputs hence, analyzing efficiency as a function of input size is essential. 

Example : sorting a longer list or multiplying larger matrices requires more time.

In many cases, the choice of parameter to represent input size is straightforward, but in some problems, the choice can significantly affect how efficiency is expressed.

#### Typical measurements of input size:

- **Sorting** – Measured by the **number of elements** (_n_) in the list or array to be sorted.
    
- **Matrix Multiplication** – For multiplying two _n × n_ matrices, common measures are:
    - **Matrix order (_n_)** – the number of rows or columns.
    - **Total number of elements (_N_)** – since _N = n²_, choosing between _n_ or _N_ can change how efficiency is expressed.
        
- **Graph Traversal** – Input size is described by **two parameters**:
    - **Number of vertices** (|V| or _n_).
    - **Number of edges** (|E| or _m_). 
    - The time complexity of BFS or DFS with adjacency lists is Θ(|V| + |E|).
        
- **Other Examples**:
          
    - **Finding the smallest element** – Measured by the list size (_n_).
        
    - **Factorial computation (_n!_)** – Measured by the integer _n_ itself.
        
    - **Polynomial evaluation** – Measured by the polynomial’s degree or number of coefficients.
    

### What are the basic operations considered when analyzing the efficiency of an algorithm?

When analyzing algorithm efficiency, the **basic operation** is the single most significant operation — the one that contributes most to the total running time. The basic operation is usually found in the **innermost loop** of the algorithm. 

Identifying it is a key step in analyzing the time complexity of both nonrecursive and recursive algorithms.
    
- **Purpose** – Counting how many times this operation executes for an input of size _n_ gives a measure of time efficiency. This count, denoted **C(_n_)**, helps estimate the total running time:
    
- **Dependence on Input** – The execution count may depend solely on input size, or also on specific properties of the input. In the latter case, **best-case, worst-case, and average-case** complexities are analyzed separately.

#### Examples of Basic Operations

**Sorting Algorithms** : basic operation is a **key comparison** (comparing elements of a list being sorted with each other)

- **Selection Sort** – Basic operation: comparison `A[i] < A[j]`.  
    Always executed (n−1)n/2 times for all inputs making it Θ(_n²_) algorithm.
    
- **Insertion Sort** – Basic operation: comparison `A[j] > v`.
    - Worst case (strictly decreasing input): (n−1)n/2 → Θ(_n²_).
    - Best case (already sorted): _n_ – 1 → Θ(_n_).
        
**Mathematical Problems** : the basic operations typically involve **arithmetical operations** such as addition, subtraction, multiplication, and division.

- **Matrix Multiplication** – Basic operation: multiplication (or addition) inside the innermost loop.  
    Definition-based method: _n³_ multiplications → Θ(_n³_).
    
- **Factorial (_n!_)** – Basic operation: multiplication in the recursive or iterative loop.
    
**Searching Algorithms** : 

- **Sequential Search** – Basic operation: key comparison. The count depends on the input; it's _n_ in the worst case (element not found or last element) and 1 in the best case (first element is the key).
	
- **Binary Search** – Basic operation: three-way comparison (less than, equal, greater than).  
    Worst case: ⌈log₂ _n_⌉ + 1 comparisons → Θ(log _n_) efficiency.
    
**Finding the Largest Element** – Basic operation: comparison `A[i] > maxval`.  
    Executed exactly _n_ – 1 times for any input size n.

**Element Uniqueness** – Basic operation: equality comparison `A[i] == A[j]`.  
    Worst case: _n(n – 1) / 2_ comparisons → Θ(_n²_).
    
### Define the "running time" of an algorithm. What factors can influence it?

The **running time** (or **time complexity**) of an algorithm measures how long it takes to complete its task. While it can be expressed in physical units (seconds, milliseconds, etc.), theoretical analysis focuses on **machine-independent measures** — primarily the **order of growth** of running time as the input size becomes large.

#### Factors Influencing Running Time

1. **Input Size (_n_)** : The most important factor: larger inputs usually take longer to process.
        
2. **Basic Operation Count** : The basic operation is the most time-consuming step (often in the innermost loop).
        
3. **Properties of the Input** : For many algorithms, the running time can depend not only on the input size but also on the particular characteristics of the input data and data distribution:
    - **Worst-Case Efficiency**: Maximum running time for any input of a given size _n_.
	
	- **Best-Case Efficiency**: Minimum running time for any input of a given size _n_.

	- **Average-Case Efficiency**: The expected running time for a "typical" or "random" input, requiring assumptions about input probability distributions.

	- **Amortized Efficiency**: Averages cost over a sequence of operations. Analyzes the total time for a sequence of operations on a data structure, distributing the cost of expensive operations over the entire sequence.
 
4. **Order of Growth (Asymptotic Complexity)** : For large _n_, growth rate of C(_n_) matters more, so constants are ignored.
    Common efficiency classes:        
	- Constant Θ(1)
	- Logarithmic Θ(log _n_)
	- Linear Θ(_n_)
	- Linearithmic Θ(_n_ log _n_)
	- Quadratic Θ(_n²_), Cubic Θ(_n³_), Exponential Ω(aⁿ).

5. **Computational Model and Hardware**
    - Most analysis assumes a **Random-Access Machine (RAM)** model with sequential execution of one after the other.
    - Modern hardware can execute **parallel algorithms**, which changes efficiency.
    - CPU speed, memory capacity, and cache performance can affect practical results.
        
6. **Implementation Quality** : Efficient coding practices and compiler optimizations can reduce time (usually by a constant factor).
        
7. **External Conditions** : In empirical testing, running time can be affected by other processes, system load, or environmental factors.


### Define what is meant by the time complexity and space complexity of an algorithm.

The efficiency of an algorithm can be measured in two principal ways: time efficiency and space efficiency. The analysis framework often focuses primarily on time efficiency. 

Both time and space efficiencies are measured as functions of the algorithm's input size.

**Time complexity**, also referred to as **time efficiency**, indicates how fast an algorithm runs. It is typically measured as a function of the algorithm's input size by counting the number of times its **basic operation** is executed. 

- The basic operation is usually the most time-consuming operation in the algorithm's innermost loop and contributes the most to the total running time. 

- This measure is preferred over using standard time units (like seconds) because it is independent of the speed of a particular computer, the quality of the implementation, and the compiler used.

**Space complexity**, or **space efficiency**, refers to the amount of memory units required by the algorithm. This includes the extra memory used by the algorithm in addition to the space needed for its input and output. 

- In the early days of computing both time and space were at a premium, advancements have made space less of a concern in many modern applications, though it still matters due to different memory tiers (main, secondary, cache).


### Explain the concepts of worst-case, best-case, and average-case efficiency analysis for algorithms.

For many algorithms, the running time is not solely dependent on the input size but also on the specific characteristics of the input. To address this, efficiency analysis distinguishes between different scenarios:

**Worst-case efficiency** refers to the algorithm's efficiency for the worst-case input of a specific size _n_, for which the algorithm runs the **longest** among all possible inputs of that size. 

- The worst-case analysis is crucial as it provides an upper bound on the running time, guaranteeing that for any instance of size _n_ , the running time will not exceed this value.

- Determining this involves analyzing the algorithm to find the input that maximizes the basic operation's count, C(n). 


**Best-case efficiency** is the algorithm's efficiency for the best-case input of size _n_ , for which the algorithm runs the **fastest** among all possible inputs of that size. 

- Analyzing this involves identifying the input for which the basic operation count C(n) is smallest. 

- While less critical than worst-case, it's not entirely useless; If the best-case is poor, the algorithm can be discarded.

- For some algorithms, a good best-case performance extends to inputs that are "almost" best-case, making them suitable for specific applications (like insertion sort for almost-sorted arrays). 


**Average-case efficiency** seeks to provide information about the algorithm's behavior on a "typical" or "random" input. 

- Analyzing this is generally more difficult than worst-case or best-case analysis. It requires making assumptions about the probability distribution of inputs of size _n_ and then calculating the expected value of the basic operation count. 

- This analysis is essential because for many important algorithms, the average-case efficiency is significantly better than the pessimistic worst-case suggests.

### Provide an example of an algorithm and discuss its best-case, worst-case, and average-case scenarios.
    
**Sequential Search** algorithm searches for a given value (a search key _K_) in a list (or array) of _n_ elements by checking the elements one after another until either the key is found or the list is exhausted. 

The basic operation is the **key comparison** (e.g., comparing `A[i]` with `K`).

```
ALGORITHM  SequentialSearch(A[0..n − 1], K)
//Searches for a given value in a given array by sequential search

//Input: An array A[0..n − 1] and a search key K
//Output: The index of the first element in A that matches K
// or −1 if there are no matching elements

i <- 0
while i < n and A[i] ≠ K do
	i <- i+1

if i < n 
	return i
else 
	return -1
```

Let's consider the algorithm `SequentialSearch(A[0..n-1], K)`:

- **Worst-case scenario:** The algorithm makes the largest number of key comparisons when the search key _K_ is **not present** in the list or when it is the **very last element** (`A[n-1]`). In either case, the algorithm has to compare the key with every element in the list. The number of comparisons is `Cworst(n) = n`.

- **Best-case scenario:** The algorithm runs fastest when the search key _K_ is found at the **very first position** of the list (`A`). Only one comparison is needed to find the key. The number of comparisons is `Cbest(n) = 1`.

- **Average-case scenario:** Analyzing the average case requires assumptions about the inputs. A standard assumption is that there's a probability _p_ (0 ≤ _p_ ≤ 1) of a successful search, and if successful, the key is equally likely to be in any position _i_ (0 to _n_-1).  The average number of comparisons `Cavg(n)` is calculated as the sum of (comparisons * probability) over all possibilities: 
- `Cavg(n) = [∑(i+1) * (p/n) for i from 0 to n-1] + [n * (1-p)]`. 
- This simplifies to `Cavg(n) = p*(n+1)/2 + n*(1-p)`. 

### Justify why worst-case analysis is often considered more important than average-case analysis in the study of algorithms.

Worst-case analysis is often considered more important due to the strong guarantees it provides.

- **Guaranteed Upper Bound:** on the algorithm's running time for any valid input of a given size. Knowing the maximum possible time ensures that the algorithm will meet performance requirements regardless of the specific input. 

- This is critical in many applications where the algorithm's performance cannot afford to degrade significantly, such as real-time systems, safety-critical software, or competitive programming where exceeding time limits means failure. 

**Limitations of Other Cases:**

- The best-case efficiency is usually not representative of typical performance, as it often occurs for specific, sometimes trivial, inputs that are unlikely to be encountered frequently in practice.

- Average-case analysis, while informative, often relies on making assumptions about the distribution of inputs. These assumptions can be difficult to validate in real-world scenarios, and if the actual input distribution differs from the assumption, the average-case prediction may be inaccurate.


