# Types of Machine Learning-2

## Unsupervised Learning: Descriptive Modeling

Unsupervised learning, often called **descriptive learning**, is a type of machine learning where the objective is to discover hidden patterns or intrinsic structures in data.

- **Input Data:** The process works on **unlabeled and unclassified data** (i.e., data without a known output, class, or label).
    
- **Core Objective:** The goal is to analyze a dataset to find **natural groupings or associations** among the data elements. This process is known as **pattern discovery** or **knowledge discovery**.
    
- **Statistical View:** In statistics, unsupervised learning is closely related to **density estimation**.
    

> **Process Flow:** Unlabeled Data → Unsupervised Learning Model → Data Patterns

### 1. Main Types of Unsupervised Learning

Unsupervised learning methods are primarily used for discovering unknown subgroups in data. The two major techniques are **Clustering** (grouping similar objects together) and **Association Analysis** (identifying patterns or relationships of attributes).

#### A. Clustering

Clustering is the primary type of unsupervised learning and involves grouping similar objects together.

- **Grouping Principle:** The goal is to partition a dataset into disjoint subsets (clusters). Objects within the same cluster should be **highly similar** (high intra-cluster similarity), while objects in different clusters should be **highly dissimilar** (low inter-cluster similarity).
    
- **Similarity Measure:** The technique relies on a similarity measure, often using **distance** metrics (e.g., Euclidean or Minkowski distance). Data items are considered part of the same cluster if the distance between them is less. This is known as distance-based clustering
    
- **Common Techniques:**
    
    1. **Partitioning Methods:** Divides data into non-overlapping subgroups (e.g., **K-Means**, **K-Medoids**).
        
    2. **Hierarchical Methods:** Builds a tree-like hierarchy of clusters, which can be **agglomerative** (bottom-up, starting with single clusters) or **divisive** (top-down, starting with one cluster).
        
    3. **Density-Based Methods:** Defines clusters as dense regions of data points (e.g., **DBSCAN**).
        
- **Performance Evaluation:** Evaluating cluster quality is often subjective, as there is no "ground truth." A popular internal measure is the **silhouette coefficient**, which measures intra-cluster homogeneity and inter-cluster heterogeneity.
    

#### B. Association Analysis (Association Rule Learning)

Association analysis focuses on identifying "if-then" relationships between data elements.

- **Goal:** To identify patterns or relationships between attributes in a dataset. It extracts rules that best explain observed relationships between variables.
    
- **Application Example:** The most common application is **Market Basket Analysis**, which finds strong associations like, "If a customer buys item A, they are also likely to buy item B."
    
- **Common Algorithm:** The **Apriori algorithm** is widely used for association rule learning.
    

### 2. Applications of Unsupervised Learning

- **Customer Segmentation:** Grouping consumers based on demography or purchasing habits.
    
- **Recommender Systems:** Identifying products or media that similar users like.
    
- **Anomaly/Fraud Detection:** Identifying unusual patterns in data (e.g., fraudulent transactions).
    
- **Document Clustering:** Grouping similar texts or articles.
    
- **Dimensionality Reduction:** Using techniques like **Principal Component Analysis (PCA)** to reduce the number of features in a dataset while retaining important information.
    

### 3. Comparison to Supervised Learning

|**Feature**|**Supervised Learning (Predictive)**|**Unsupervised Learning (Descriptive)**|
|---|---|---|
|**Data Type**|**Labeled** training data (known output)|**Unlabeled** data (no known output)|
|**Objective**|Predict the class or value of unknown objects|Find groups, structures, or patterns|
|**Output Variable**|An explicit target variable ($Y$) exists|No target variable; input features ($X$) only|
|**Examples**|Classification, Regression|Clustering, Association Analysis, PCA|

## Reinforcement Learning (RL)

Reinforcement learning (RL) is a type of machine learning where an **agent** (the model) learns to make optimal decisions by interacting with an **environment**.

It is used for solving problems where **decision-making is sequential** and the goal is **long-term** (e.g., game playing or resource management). The agent learns by itself through a **penalty/reward mechanism**; it receives **rewards** for actions that lead to good outcomes and **penalties** (or no reward) for those that do not.

### 1. The Reinforcement Learning Framework

RL attempts to emulate "learning by self" (trial and error).

- **Analogy (Child Learning to Walk):**
    
    - The child is the **agent**.
        
    - The goal is the task (walking).
        
    - The floor with hurdles is the **environment**.
        
    - Successfully taking a step is a **reward**.
        
    - Falling is a **penalty** (negative reward).
        

### 2. The Markov Decision Process (MDP)

RL problems are formally described using the **Markov Decision Process (MDP)**, which consists of a quadruplet $E = \langle X, A, P, R \rangle$:

- $X$: The set of all possible **states** (a description of the environment).
    
- $A$: The set of all possible **actions** the agent can perform.
    
- $P$: The **transition function** (the probability of moving from one state to another given an action).
    
- $R$: The **reward function** (the reward returned by the environment after a state transition).
    
The agent influences the environment by taking actions but only perceives the environment by observing transited states and returned rewards.

### 3. The Objective: Learning a Policy

The agent's goal is to learn a **policy ($\pi$)**, which is a "strategy" that selects the best action to take in any given state.

- **Policy Representation:** A policy can be deterministic ($\pi: X \rightarrow A$, mapping a state to a single action) or stochastic ($\pi: X \times A \rightarrow [0, 1]$, mapping a state-action pair to a probability).
    
- **Objective:** The objective of RL is to find a policy $\pi$ that **maximizes the long-term cumulative rewards**.
    

### 4. Comparison with Supervised Learning

While both use "feedback," the mechanism is different:

- **Sequential Decisions:** RL handles sequential, long-term goals, whereas supervised learning typically involves single-shot predictions.
    
- **Time-Delayed Labels:** In supervised learning, the label (feedback) is immediate and explicit. In RL, the feedback (reward) is often **time-delayed**; an agent may not know if an action was "good" or "bad" until many steps later.
    
-   **No Labels:** In RL, there are no labeled samples to tell the agent what to do for a given state.

### 5. RL Sub-Paradigms and Techniques

- **Model-Based vs. Model-Free:**
    
    - **Model-Based:** The agent knows or learns the environment's model ($P$ and $R$).
        
    - **Model-Free:** The agent does _not_ know the model and learns purely from trial and error (e.g., **Q-Learning**).
        
- **Exploration vs. Exploitation:** The agent must balance **Exploitation** (using the best action it _knows_) with **Exploration** (trying new actions to _discover_ if they are better). This is studied in the **K-Armed Bandit** problem.
    
- **Imitation Learning:** Learning from examples of decisions provided by human experts (demonstrations).
    
    - **Inverse Reinforcement Learning (IRL):** Deriving the (unknown) reward function by observing an expert.
        
- **Reinforcement Learning from Human Feedback (RLHF):** An alignment technique, critical to modern Large Language Models (LLMs), where a model is trained using feedback (rewards) generated by human overseers to align its behavior with human expectations.
    

### 6. Applications

- **Game Playing:** Intelligent game bots (e.g., Google's **AlphaGo**).
    
- **Self-Driving Cars:** Making real-time decisions about speed, steering, and braking.
    
- **Robotics:** Training robots to navigate complex environments or manipulate objects.
    
- **Resource Management:** Optimizing energy consumption, logistics, or financial trading.
    
## Comparison: Supervised, Unsupervised, and Reinforcement Learning

| **Feature**    | **Supervised Learning**                    | **Unsupervised Learning**                           | **Reinforcement Learning**                                    |
| -------------- | ------------------------------------------ | --------------------------------------------------- | ------------------------------------------------------------- |
| **Model Type** | Predictive Model                           | Descriptive Model                                   | Policy/Agent Learning                                         |
| **Input Data** | **Labeled** training data.                 | **Unlabeled** data.                                 | State $X$, Action $A$, and Reward $R$ from the environment.   |
| **Core Goal**  | Prediction (class or value).               | Pattern/Group Discovery.                            | Maximizing long-term cumulative rewards.                      |
| **Mechanism**  | Guided learning from labeled input.        | Self-discovery of intrinsic relationships.          | Trial-and-error using a **reward/penalty** system.            |
| **Feedback**   | Immediate (label provided in data).        | None / Implicit in data structure.                  | **Time-delayed** (received after actions lead to outcomes).   |
| **Output**     | A Classification or Regression model.      | Clusters, Association Rules, or reduced dimensions. | An optimal **Policy** (a function mapping states to actions). |
| **Examples**   | Tumor prediction, stock price forecasting. | Customer segmentation, Market Basket Analysis.      | Self-driving cars, game playing (AlphaGo).                    |

## Other Learning Types

### Semi-Supervised Learning

- A hybrid approach that uses **both labeled and unlabeled data** for training, typically a small amount of labeled data and a large amount of unlabeled data.
    
- The goal is the same as supervised learning (prediction), but it leverages the unlabeled data to improve model accuracy.
    

### Active Learning

- A form of supervised learning applied when obtaining labeled examples is expensive or time-consuming.
    
- The learning algorithm **interactively queries the user** to obtain labels for new data points that it calculates would be the _most informative_ for improving its model.