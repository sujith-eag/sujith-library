# Types of Machine Learning-1

Machine learning problems are broadly classified into three primary categories. Sometimes, **Semi-Supervised Learning** (a hybrid of supervised and unsupervised) is included as a fourth category.

| **Category**               | **Common Name**      | **Primary Goal**                                                                             | **Learning Mechanism**                                           |
| -------------------------- | -------------------- | -------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| **Supervised Learning**    | Predictive Learning  | **Prediction** of a class or value for unknown data based on prior labeled data.             | Learning from **labeled** training data (guided by a "teacher"). |
| **Unsupervised Learning**  | Descriptive Learning | **Discovery** of underlying groups, patterns, or structures in unlabeled data.               | Learning from **unlabeled** data (self-discovery).               |
| **Reinforcement Learning** | Agent Learning       | **Action** to achieve a goal in an environment, guided by a system of rewards and penalties. | Learning by self (trial and error).                              |

## Supervised Learning

**Experience as Labeled Data:** Supervised learning (SL) is a type of machine learning defined by its requirement for **labeled training data**. In the context of the universal ML definition, this labeled data serves as the **experience ($E$)** on a specific task a machine has to execute.

For example, to solve an image segregation problem, the training data would contain features of many images, with each image having a **label** (e.g., "round" or "triangular"). Based on this, the machine builds a predictive model to assign labels to new, unlabeled test data.


> Labelled training data -> Supervised learning -> Prediction Model -> Test Data -> Prediction

### 1. Definition and Core Concept

The fundamental goal of supervised learning is to build a statistical model that learns a mapping function from inputs to outputs.

- **Guided Learning:** The process is "supervised" because the algorithm learns from a training set that is analogous to a teacher providing correct answers (the labels).
    
- **Mathematical Mapping:** The algorithm seeks to learn a relationship, or mapping function, $f: X \rightarrow Y$ from the input space ($X$, or **predictor features**) to the output space ($Y$, or the **target feature**).
    
- **Input Data:** The "experience" is provided as a training set of $N$ examples: $\{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$, where $x$ is the input feature vector and $y$ is the known output label.
    

### 2. Sub-Types of Supervised Learning

Supervised learning problems are categorized based on the nature of their output variable ($Y$).

#### A. Classification

Classification is used when the output variable is **categorical or nominal** (i.e., it belongs to a finite set of discrete classes or labels).

- **Objective:** To assign a **label, category, or class** to a new, unseen instance.
    
- **Output Type:** Categorical (Nominal / Discrete). The categories are often called **levels**.
    
- **Types:**
    
    - **Binary Classification:** The output has only two possible classes (e.g., "Spam" / "Not Spam," "Malignant" / "Benign").
        
    - **Multiclass Classification:** The output has more than two classes (e.g., Handwriting recognition (0-9), "Round" / "Triangular" / "Square").
        
- **Examples:** Email spam filtering, image classification, fraud detection, medical diagnosis.

Classification is a type of supervised learning where a target feature, which is of type categorical, is predicted for test data based on the information imparted by training data. The target categorical feature is known as class.

> Labelled training data -> Classifier -> Classification Model -> Test Data -> Intel

#### B. Regression

Regression is used when the output variable is a **continuous** (i.e., a real-valued number) and not a class.

- **Objective:** To predict a **numerical feature** or **real-valued output**.
    
- **Output Type:** Predictor variable and the target variable are Continuous (Numerical / Real-valued).
    
- **Examples:** Predicting real estate prices, stock market values, temperature, or sales revenue.
    
- **Common Algorithm:** **Linear Regression** is a common algorithm that fits a straight-line relationship between the predictor and target variables using the _least squares method_.
    
    - **Simple Linear Regression:** Uses one predictor variable. The model is represented as $y = a + bx$. where ‘x’ is the predictor variable and ‘y’ is the target variable.
        
    - **Multiple Linear Regression:** Uses multiple predictor variables.
        
> **Example:** A sales manager predicts next year's sales revenue (a continuous value) based on the previous year's sales and investment figures (also continuous values). This is a regression problem.

### 3. Steps in the Supervised Learning Workflow

1. **Problem Identification:** Define the business problem and the goal of the model.
    
2. **Data Sourcing and Collection:** Identify and gather the required data, ensuring it accurately represents the problem.
    
3. **Define Training/Test Sets:** Decide on the data configuration, typically splitting the data into training and testing sets.
    
4. **Data Pre-processing:** Clean, transform, and prepare the data (e.g., handling missing values, feature scaling).
    
5. **Algorithm Selection:** Choose a suitable learning algorithm (e.g., Decision Tree, SVM). This is often considered the **most critical step**.
    
6. **Model Training:** Run the algorithm on the training data. This may involve tuning **hyperparameters** (control parameters) to optimize the model.
    
7. **Model Evaluation:** Measure the trained model's performance on the unseen test data. If results are unsatisfactory, return to step 5 or 6.
    

### 4. Common Supervised Learning Algorithms

#### i. Decision Tree

- **Mechanism:** Makes decisions using a tree-like structure. The learning process splits the data based on features that provide the most information, often measured using concepts like **Information Gain** or **Entropy**.
    
- Decision trees are generally considered **non-parametric** models, meaning the number of parameters grows with the size of the training data.
    

#### ii. k-Nearest Neighbour (kNN)

- **Philosophy:** "An instance is likely to be similar to its nearby neighbors."
    
- **Mechanism:** Classifies a new data point by a **majority vote of its $k$ nearest neighbors** in the training data, based on a distance metric (e.g., Euclidean distance).
    
- **Type:** kNN is a classic example of **instance-based learning** or a **lazy learner**.

- **Learning Type (Lazy Learner):** kNN is classified as **instance-based learning** or **lazy learning**. Lazy learners completely skip the abstraction and generalization processes and use the training data "as-is" for classification, requiring very little time for training but more time for classification of new instances.


#### iii. Support Vector Machines (SVM)

- **Objective:** A powerful classifier that finds a **separating hyperplane** (a decision boundary) in a high-dimensional space.
    
- **Margin Maximization:** The goal is to find the hyperplane that **maximizes the margin** (the distance) between the closest data points (the **support vectors**) of the different classes, which promotes better generalization.
    
- **Kernel Trick:** Uses **kernels** (e.g., linear, polynomial, RBF) to transform non-linearly separable data into a higher dimension where a linear separator can be found.
    

#### iv. Naïve Bayes Classifier

- **Mechanism:** A probabilistic classifier based on **Bayes' Theorem** of conditional probability.
    
- **"Naïve" Assumption:** It operates on the "naïve" (and simplifying) assumption that all predictor features are **conditionally independent** of one another given the class. Despite this simplicity, it is often very effective.
    

#### v. Ensemble Learning (e.g., Random Forest)

- **Mechanism:** An **ensemble method** that combines multiple "weaker" individual models to create a single, "stronger" prediction model. This averages out biases and reduces variance.
    
- **Random Forest:** A popular ensemble model that operates by constructing a multitude of **Decision Trees** during training.
    

### 5. Related Concepts

- **Model Selection:** The process of partitioning data to train and validate models.
    
    - **Holdout Method:** Typically splits data into 70%–80% for training and 20%–30% for testing.
        
    - **K-fold Cross-validation:** A more robust method where the data is divided into $k$ folds, and the model is trained and validated $k$ times.
        
- **Eager vs. Lazy Learning:**
    
    - **Eager Learners:** (e.g., SVM, Decision Trees). These algorithms construct a generalized model from the training data _during the training phase_. The original training data can then be discarded.
        
    - **Lazy Learners:** (e.g., kNN). These algorithms store the entire training dataset. They perform no abstraction during training and only do the work of generalization _at the time of classification_. They typically have fast training times but slower prediction times.