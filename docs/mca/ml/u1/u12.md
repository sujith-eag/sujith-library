# Types of Machine Learning

Machine learning problems can generally be classified into three broad categories: Supervised Learning, Unsupervised Learning, and Reinforcement Learning.

Sometimes, **Semi-Supervised Learning** is listed as a fourth category.

| Category | Common Name | Primary Goal | Learning Mechanism |
| :--- | :--- | :--- | :--- |
| **Supervised Learning** | Predictive learning. | **Prediction** of class/value of unknown objects based on prior labeled data. | Learning from **labeled** training data (teacher/expert guidance). |
| **Unsupervised Learning** | Descriptive learning. | **Discovery** of underlying groups or patterns in unlabelled data. | Learning from **unlabelled** data (self-discovery). |
| **Reinforcement Learning** | Agent learning. | Learning to act autonomously to achieve goals by using a system of rewards and punishments. | Learning by self (trial and error). |

## Supervised Learning

Supervised learning (SL) is defined by the requirement for **labeled training data** as input. This basic input, or the experience in the paradigm of machine learning, is given in the form of training data which is the past information on a specific task.

In the context of an image segregation problem, training data will have past data on different features of a number of images, along with a tag on whether the image is round or triangular. The tag is called a ‘label’, and we say that the training data is labelled in supervised learning.

> Labelled training data -> Supervised learning -> Prediction Model -> Test Data -> Prediction

### 1. Definition and Core Concept

The fundamental goal of supervised learning is to build a model that takes a feature vector $x$ as input and outputs information that allows deduction of the label.

-   **Experience as Labeled Data:** In the context of the universal ML definition (Mitchell's definition), the past information or experience ($E$) provided to the machine is in the form of **labeled training data**.
-   **Guided Learning:** This process is called supervised learning because the learning mechanism is analogous to a student being **supervised** by a teacher (the labeled data).
-   **Mathematical Mapping:** The learning algorithm attempts to establish a relationship between the target feature (the output variable) and the predictor features (the input variables). This establishes a mapping $f: X \rightarrow Y$ from the input space $X$ to the output space $Y$.

### 2. Steps in Supervised Learning

The classification learning steps are critical for implementing supervised models:

1.  **Problem Identification:** Defining a well-formed problem with clear goals and long-term impact.
2.  **Identification of Required Data:** Evaluating the necessary data set that accurately represents the problem.
3.  **Definition of Training Data Set:** Deciding the specific data configuration to be used as a training set.
4.  **Data Pre-processing:** Cleaning or transforming the data set.
5.  **Algorithm Selection:** Determining the structure of the learning function; this is considered the **most critical step**.
6.  **Training:** Running the algorithm on the gathered training set, which may involve tuning hyperparameters (control parameters).
7.  **Evaluation with Test Data Set:** Measuring the model's performance on unseen data. If results are unsatisfactory, further training or parameter tuning may be required.

### 3. Sub-Types of Supervised Learning

Supervised learning problems are categorized based on the nature of the output variable:

#### A. Classification

Classification is a type of supervised learning where a target feature, which is of type categorical, is predicted for test data based on the information imparted by training data. The target categorical feature is known as class.

-   **Objective:** To assign a **label, category, or class** to a test data instance.
-   **Output Type:** The output variable is **categorical** (nominal), meaning it is **discrete**. The categories are often called **levels**.
-   **Binary vs. Multiclass:** If the output has two possible classes (e.g., "win" or "loss"), it is **binary classification** (or binomial). If there are more than two classes, it is **multiclass classification** (or multinomial).
-   **Examples:** Predicting whether a tumor is malignant or benign, classifying emails as spam or non-spam, handwriting recognition, identifying potential fraudulent transactions,  image classification and prediction of natural calamity like earthquake, flood, etc.

The machine maps a new image (test data) to a set of images to which it is similar (in training data) and assigns the same label or category to the test data.

> Labelled training data -> Classifier -> Classification Model -> Test Data -> Intel

Common classification algorithms include:
-   Naïve Bayes
-   Decision tree
-   k-Nearest Neighbour

Trying to predict which category or class an unknown data belongs to, predicting a categorical or nominal variable, the problem is known as a classification problem. 

The whole problem revolves around assigning a label or category or class to a test data based on the label or category or class information that is imparted by the training data.

#### B. Regression

Regression is used when trying to predict an absolute value (a real-valued variable) and not a class.

-   **Objective:** To predict a **numerical feature** or **real-valued output**.
- **Output Type:** The underlying predictor variable and the target variable are **continuous** in nature.
- **Examples:** Predicting real estate price, stock price, temperature, marks in an examination, or sales revenue. Demand forecasting in retails, weather forecast, skill demand forecast in job market.

-   **Algorithms:** Common algorithms include **Simple Linear Regression** and **Multiple Linear Regression**.
    -   In simple linear regression, there is only one predictor variable.
    -   In multiple linear regression, multiple predictor variables can be included in the model.

In linear regression, a straight-line relationship is ‘fitted’ between the predictor variables and the target variables, using the statistical concept of the least squares method. The goal is to minimize the sum of the square of the error between actual and predicted values.

A typical linear regression model can be represented as:
* $y = a + bx$
...where ‘x’ is the predictor variable and ‘y’ is the target variable.


Yearly budgeting of the sales managers giving sales prediction for the next
year based on sales figure of previous years vis-à-vis investment being put in. The data related to past as well as the data to be predicted are continuous in nature. A simple linear regression model can be applied with investment as predictor variable and sales revenue as the target variable.

### 4. Common Supervised Learning Algorithms

#### i. Decision Tree

-   Decision trees make decisions based on **tree structures**.
- The learning process simulates tree-based decision processes by relying on **information theory** (e.g., maximizing Information Gain or minimizing Entropy).
- Decision trees are generally considered **non-parametric** models, meaning the number of parameters grows with the size of the training data.
- Algorithms include ID3, C4.5, and CART.

#### ii. k-Nearest Neighbour (kNN)

-   **Philosophy:** "Neighbours in a locality have a similar background".
-   **Mechanism:** An unknown data instance is classified by a **majority vote of its $k$ nearest neighbors** in the training data, based on a distance metric (e.g., Euclidean distance).
- **Learning Type (Lazy Learner):** kNN is classified as **instance-based learning** or **lazy learning**. Lazy learners completely skip the abstraction and generalization processes and use the training data "as-is" for classification, requiring very little time for training but more time for classification of new instances.

#### iii. Support Vector Machines (SVM)

-   **Goal:** SVM is a powerful classifier whose objective is to find a **separating hyperplane** that maximizes the **margin** between samples of different classes, providing strong **generalization ability**.
- **Maximum Margin:** The margin is the distance between the closest examples of the two classes, as defined by the decision boundary. A larger margin contributes to better generalization. SVM minimizes the norm of the weight vector $w$ to maximize this margin.
- **Support Vectors:** These are the critical data points nearest the hyperplane; if removed, they would alter the position of the dividing hyperplane.
- **Kernel Trick:** SVM uses functions called **kernels** to transform data from a lower-dimensional input space to a higher-dimensional space, thereby converting **non-linearly separable** data into linearly separable data. Common kernels include linear, polynomial, sigmoid, and Gaussian RBF kernels.

#### iv. Naïve Bayes Classifier

- This classifier is based on the **Bayes’ theorem** of conditional probability.
- It is considered a simple but powerful technique for assigning class labels to problem instances.
- It operates under the simplifying assumption that the attribute values are **conditionally independent** given the classification of the instance (hence the term "Naïve").

#### v. Ensemble Learning (e.g., Random Forest)

- **Ensemble methods** combine multiple individual learners ("weaker models") to create a single, **stronger prediction model**. This approach averages out the biases of different underlying models and reduces variance.
- **Random Forest** is an ensemble classifier that uses and combines **many Decision Tree models**.

### 5. Other Related Supervised Concepts

- **Model Selection and Training:** In supervised learning, the data set is typically divided into **training data** (for learning parameters) and **test data** (for evaluating performance). Techniques like the **Holdout Method** (using 70%–80% for training) or **K-fold Cross-validation** are used to partition the data.

-   **Eager vs. Lazy Learning:**
    -   **Eager learners** (like SVM, Decision Trees) construct a generalized model during training, allowing the training data to be discarded afterward.
    -   **Lazy learners** (like kNN) store all training examples and rely on them for classification, essentially using the entire dataset as the model.


## Unsupervised Learning: Descriptive Modeling

Unsupervised Learning is often referred to as **descriptive learning**. Its central premise is the discovery of hidden knowledge from data that lacks external guidance.

The learning is performed **without any prior training** or labeled samples. This is used when there is **no idea about the class or label** of a particular data.

-   **Input Data:** The process works on **unlabelled and unclassified information**.
-   **Core Objective:** The objective is to analyze a dataset to find **natural groupings or patterns** within the data elements or records.
-   **Goal:** The intention is to find the **association between the features or their grouping** to understand the nature of the data. This process is known as **pattern discovery or knowledge discovery**.

- **Statistical View:** In statistical terms, unsupervised learning is closely related to **density estimation**.

### 1. Main Types of Unsupervised Learning

Unsupervised learning utilizes methods for **discovering unknown subgroups** in data. The two major techniques are **Clustering** (grouping similar objects together) and **Association Analysis** (identifying patterns or relationships of attributes).

#### A. Clustering

Clustering is the primary type of unsupervised learning and involves grouping similar objects together.

-   **Grouping Principle:** Clustering aims to partition a data set into disjoint subsets (clusters). Objects in the same cluster should be **similar** (or related to each other) to each other but **dissimilar** (or unrelated) to objects from other groups.
-   **Similarity Measure:** This technique relies on determining similarity, often using **distance** metrics (e.g., Minkowski distance). Data items are considered part of the same cluster if the distance between them is less known as distance-based clustering
-   **Clustering Techniques:**
    1.  **Partitioning methods:** Divides data into non-overlapping subgroups (e.g., **K-Means**, **K-Medoids**).
    2.  **Hierarchical clustering:** Builds a hierarchy of clusters (either **agglomerative** (bottom-up, starting with single clusters) or **divisive** (top-down, starting with one cluster)).
    3.  **Density-based methods:** Defines clusters as dense regions of data points (e.g., **DBSCAN**).
-   **Performance Evaluation:** Evaluating cluster quality is often subjective. A popular internal measure is the **silhouette coefficient**, which measures intra-cluster homogeneity and inter-cluster heterogeneity.

#### B. Association Analysis (Association Rule Learning)

Association analysis focuses on identifying relationships between data elements.

-   **Goal:** The technique identifies a **pattern or relationship of attributes** within the data set. It extracts rules that best explain observed relationships between variables in data.
-   **Application Example:** The most common example is **Market Basket Analysis**, which looks for strong associations like "purchase of item A" occurring alongside "purchase of item B".
-   **Algorithm:** The **Apriori algorithm** is commonly used for association rule learning.

> k-means, Principal Component Analysis(PCA), Self-Organizing map(SOM), Apriori algorithm, DBSCAN

> Unlabelled data -> Unsupervised Learning Model -> Data Patterns

### 2. Applications of Unsupervised Learning

Unsupervised learning is highly valuable due to its flexibility in working with un-categorized data.

-   **Customer Segmentation:** Grouping consumers based on demography or purchasing habits.
-   **Recommender Systems:** Often driven by association analysis.
-   **Anomaly/Fraud Detection:** Identifying anomalous patterns in data, such as loan defaulters in the banking sector.
-   **Image Processing and Document Clustering:** Grouping similar texts or images (facial recognition).
-   **Dimensionality Reduction:** Techniques like **Principal Component Analysis (PCA)** reduce the dimensionality of sample data,  simplifying subsequent modeling and analysis.

### 3. Comparison to Supervised Learning

| Feature             | Supervised Learning (Predictive)              | Unsupervised Learning (Descriptive)           |
| :------------------ | :-------------------------------------------- | :-------------------------------------------- |
| **Data Type**       | Labeled training data (known output)          | Unlabeled data (no known output)              |
| **Objective**       | Predict the class or value of unknown objects | Find groups, structures, or patterns          |
| **Output Variable** | Explicit target variable ($Y$) exists         | No target variable; input features ($X$) only |
| **Examples**        | Classification, Regression                    | Clustering, Association Analysis              |

---

## Reinforcement Learning (RL)

Reinforcement learning is the type of ML where a machine learns to act on its own to achieve given goals.

RL is used for solving problems where **decision making is sequential** and the goal is **long-term** (e.g., resource management or game playing).

- The process is described using a **Markov Decision Process (MDP)**, where an agent interacts with an environment.
- The machine learns by itself through a **penalty/reward mechanism** and updates itself through reward/punishment.
- There are **no labeled samples** to tell the agent which action it should take for a given state; instead, the agent receives feedback only after the outcome (supervised learning with time-delayed labels).
- RL problems can be categorized into **Model-Based Learning** (model $E$ is known) and **Model-Free Learning** (model $E$ is unknown).


### 1. Core Mechanism and Analogy

RL attempts to emulate **"Learning by self"** in humans.

-   **Learning Process:** Learning occurs through continuous observation and correction, typically by learning from past mistakes.
-   **Reward/Penalty System:** When a sub-task is accomplished successfully, a **reward** is given. If not, **no reward** (or a penalty) is given. This system guides the machine.
-   **Analogy (The Child Learning to Walk):**
    -   The child is the **agent**.
    -   The goal is the task (walking).
    -   The place with hurdles is the **environment**.

### 2. Markov Decision Process (MDP) Framework

Reinforcement learning problems are usually described using the **Markov Decision Process (MDP)**. 

The MDP involves a quadruplet $E = \langle X, A, P, R \rangle$:
-   $X$: The **state space** (description of the environment).
-   $A$: The **action space** (actions the agent can perform).
-   $P$: The **transition function** (dictates the state transition probability when action is performed).
-   $R$: The **reward function** (dictates the reward returned by the environment after a state transition).

The agent influences the environment by taking actions but only perceives the environment by observing transited states and returned rewards.

### 3. Policy and Objective

The agent's goal is to learn a **policy ($\pi$)** that selects the action $a = \pi(x)$ at state $x$.

- **Policy Representation:** Policies can be represented as functions for deterministic policies ($\pi: X \rightarrow A$) or as probabilities for stochastic policies ($\pi: X \times A \rightarrow R$), where the probabilities of all possible actions must sum to 1.
- **Objective:** The objective of RL is to find a policy that **maximizes the long-term cumulative rewards**.
- **Cumulative Rewards:** Cumulative rewards are typically calculated either as $T$-step cumulative rewards or $\gamma$-discounted cumulative rewards.

### 4. Comparison with Supervised Learning

-   **Sequential Decision Making:** RL solves problems where decision making is sequential and goal is long-term.
-   **No Labels:** In RL, there are no labeled samples to tell the agent what to do for a given state.
-   **Time Delay:** The agent receives feedback only after the outcome (supervised learning with **time-delayed labels**).

### 5. RL Sub-Paradigms and Techniques

-   **Model-Based Learning:** Assumes the MDP quadruplet ($E$) is **known**. Solved using dynamic programming.
-   **Model-Free Learning:** The environment model ($E$) is **unknown**. Techniques include **Monte Carlo (MCRL)** and **Temporal Difference (TD) learning**.

Key related techniques include:
-   **K-Armed Bandit:** Analyzes the trade-off between **Exploration** (trying new actions) and **Exploitation** (using the best known action).
- **Imitation Learning:** This involves learning from examples of decisions provided by **human experts** (demonstrations).
    - **Direct Imitation Learning** (Behavior Cloning): Imitating state-action pairs provided by experts.
    - **Inverse Reinforcement Learning:** Deriving the **reward function inversely** from provided examples.
-   **Q-Learning:** A popular model-free RL algorithm.
- **Reinforcement Learning from Human Feedback (RLHF):** An alignment technique that involves training a model based on feedback (rewards or penalties) from human overseers, guiding the model's behavior to align with human expectations. This approach has led to significant improvements in modern LLMs like ChatGPT.


### 6. Applications

RL is applied across numerous complex domains that require sequential decision-making:

- **Self-Driving Cars:** The car must manage real-time critical information (speed, traffic, weather) and execute tasks (accelerate/decelerate, turn).
- **Game Playing:** Intelligent robots and game algorithms, such as Google's **AlphaGo**, which used RL to defeat the best human Go player.
- **Robotics:** Navigating and coordinating robots.
- **Logistics and Resource Management:** Optimization of complex electric power systems or inventory management.

---

## Comparison: Supervised, Unsupervised, and Reinforcement Learning

The critical difference lies in the **input data** and the **objective** of the learning process.

| Feature             | Supervised Learning                                                   | Unsupervised Learning                          | Reinforcement Learning                                      |
| :------------------ | :-------------------------------------------------------------------- | :--------------------------------------------- | :---------------------------------------------------------- |
| **Model Type**      | Predictive Model                                                      | Descriptive Model                              | Policy/Agent Learning                                       |
| **Input Data**      | **Labeled** training data. (Known output or class field is provided). | **Unlabeled** data. (No known class or value). | State $X$, Action $A$, and Reward $R$ from the environment. |
| **Core Goal**       | Prediction (class or value).                                          | Pattern/Group Discovery (natural groupings).   | Maximizing long-term cumulative rewards.                    |
| **Mechanism**       | Guided learning from labeled input.                                   | Self-discovery of intrinsic relationships.     | Trial-and-error using **reward/penalty** system.            |
| **Feedback Timing** | Immediate (label provided in training data).                          | None needed/Implicit.                          | Time-delayed (received after actions lead to outcome).      |
| **Output**          | Classification (categorical) or Regression (continuous).              | Clusters or Association Rules.                 | Optimal Policy (function $f(x)$ outputting best action).    |
| **Examples**        | Tumor prediction, stock price.                                        | Customer segmentation, Market Basket Analysis. | Self-driving cars, game playing.                            |


## Other Learning Types

### Semi-Supervised Learning

-   Contains **both labeled and unlabeled examples**, usually with a much higher quantity of unlabeled examples.
-   It uses a small amount of labeled data along with unlabelled data for training.
-   The goal is the same as supervised learning, but leveraging unlabeled data is intended to produce a better model.

### Active Learning

-   A supervised learning paradigm applied when obtaining labeled examples is **costly**.
-   The learner **interactively queries the user** to obtain desired outputs for new data points that contribute the most to model quality.
