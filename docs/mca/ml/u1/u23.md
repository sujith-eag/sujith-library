# Chapter 2: Preparing to Model

## 2.5.2 Data Remediation

Data issues in quality need to be **remediated** if the goal is to achieve the right amount of efficiency in the learning activity. Proper remedial steps must be taken, particularly for issues arising from human errors, such as outliers and missing values.

**Major Data Quality Issues Requiring Remediation** The need for remediation stems from common data problems encountered in the process of preparing to model:

1. **Missing Values:** Certain data elements are without a value or possess a missing value.
2. **Outliers:** Data elements have values that are surprisingly different from the other values in that attribute.

**Strategies for Handling Missing Values and Outliers** Remediation activities involve finding potential issues in data and performing necessary fixes, such as imputing missing data values. The primary measures for remediating outliers and missing values include:

1. **Removal:** Removing specific rows that contain outliers or missing values.
2. **Imputation (Standard Values):** Imputing the missing value with a standard statistical measure for that attribute, such as the mean, median, or mode.
3. **Estimation (Similar Records):** Estimating the missing value based on the values of that attribute in records considered similar, and then replacing the missing value with the estimated value.
    - For example, if the weight of a Russian student (age 12, height 5 ft.) is missing, the weight of another Russian student with a similar age and height could be assigned.

_(Note: Remedying data issues stemming from certain data elements without value or missing value can also involve proper sampling techniques, although this area is usually covered as a specialized subject area in statistics and is generally not covered in detail in this resource)._

## 2.6 Data Pre-Processing

**Data Pre-Processing** is a collective term for the **transformations applied to the identified data before feeding it into the algorithm**. This step ensures that the data is prepared for modeling.

The main data pre-processing activities discussed are:

1. Dimensionality reduction.
2. Feature subset selection.

### 2.6.1 Dimensionality Reduction

Dimensionality reduction is a technique used to **reduce the number of attributes or features** in the data set.

**Motivation: High-Dimensional Data**

- Historically, data sets used in machine learning typically consisted of only a few tens of attributes.
- In the last two decades, there has been a rapid increase in **high-dimensional data sets**.
    - Domains like **computational biology** (e.g., genome projects) frequently produce data sets with **20,000 or more features**.
    - The widespread adoption of social networking and the subsequent need for text classification also contributes to this.
- Feature transformation is considered an effective tool for **dimensionality reduction**, which consequently helps in **boosting learning model performance**.
- **PCA** (Principal Component Analysis) is noted as one technique that can be used for dimensionality reduction.

### 2.6.2 Feature Subset Selection

Feature subset selection is the process that intends to **select a subset of system attributes or features** that make the most meaningful contribution to the machine learning activity.

- This process is arguably the **most critical pre-processing activity** in any machine learning project.
- It serves as a method for dimensionality reduction.
- **The objective of feature selection is threefold**:
    1. Achieving a faster and more cost-effective learning model (i.e., less need for computational resources).
    2. Improving the efficiency of the learning model.
    3. Gaining a better understanding of the underlying model that generated the data.
- For example, when predicting student weight, eliminating an irrelevant feature like "Roll Number" helps build a feature subset that is expected to give better results than the full set.
