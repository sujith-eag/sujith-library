# 3.6 Improving Model Performance

## Context and Objective

This section addresses the procedures for enhancing a model's performance _after_ the initial phases of selection, training, and evaluation are complete. The primary objective is to understand the techniques available to boost model effectiveness, primarily by adjusting its configurable parameters or by combining it with other models.

Before focusing on performance improvement, the initial model selection must be finalized. Model selection is influenced by several aspects:

- The type of learning task (e.g., supervised, unsupervised).
    
- The nature of the data (e.g., categorical, numerical).
    
- The specific problem domain.
    

## Primary Avenues for Performance Improvement

Once a baseline model is established, there are two primary avenues for improving its performance: adjusting the model's internal settings (Hyperparameter Tuning) or combining it with other models (Ensemble Learning).

### 1. Hyperparameter Tuning (Model Parameter Tuning)

This is the process of adjusting the model's "fitting options," which are more formally known as **hyperparameters**. These are the user-configurable parameters of the learning algorithm, as opposed to the internal _model parameters_ (like weights) that are learned from the data.

- **Mechanism:** The goal of tuning is to find the optimal combination of hyperparameter values that achieves the best **balance between bias and variance**, leading to the best generalization on unseen data.
    
- **Application:** Nearly all machine learning models have at least one hyperparameter that can be tuned.
    
- **Examples:**
    
    - **k-Nearest Neighbour (kNN):** The hyperparameter is **'$k$'** (the number of neighbors to consider) can be adjusted to tune the performance and perform a trade-off between bias and variance. 
    - A small $k$ can lead to high variance (overfitting), while a large $k$ can lead to high bias (underfitting).
        
    - **Neural Networks:** Key hyperparameters include the **number of hidden layers**, the number of neurons per layer, the learning rate, and the activation function.
        

### 2. Ensemble Learning

Ensemble learning is an alternative approach that increases performance by **combining the predictions of several individual models**.

- **Core Principle:** The technique functions best when the combined models are **complementary**. This means the models are diverse and make different kinds of errors; the weakness of one model is offset by the strength of another.
    
- **Function:** Ensemble methods effectively combine multiple "weaker learners" (models that perform slightly better than random guessing) to create a single, robust "stronger learner."
    
- **Benefits:**
    
    - Helps in **averaging out the biases** of the different underlying models.
        
    - Aids in **reducing the variance**, making the final model more stable and less sensitive to the specific training data.
        
    - Often yields a significant performance boost, even if the component models are built conventionally.
        

> **Conceptual Analogy**
> 
> Improving model performance is like optimizing a car for a race.
> 
> - **Hyperparameter Tuning** is like adjusting the individual parts of a single engine (e.g., spark plug gap, fuel injection timing) to get the best speed and stability.
>     
> - **Ensemble Learning** is like combining the best features of several different engines—or running multiple specialized cars (one for corners, one for straights)—and blending their results to get the best overall race time.
