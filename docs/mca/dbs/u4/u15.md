# Big Data

The term **Big Data** refers to datasets whose **size and complexity exceed** the capabilities of traditional database management systems (DBMSs) to **capture, store, manage, and analyze** efficiently. 

It is not defined by a specific size, but rather by the **challenges** posed by the data in terms of **volume, velocity, variety**, and other associated properties.

In the modern data environment, Big Data can range in size from **terabytes** (10¹² bytes) to **petabytes** (10¹⁵ bytes), and even **exabytes** (10¹⁸ bytes), depending on the use case.

___

Big Data is characterized by several key dimensions, often referred to as the "Vs" : **Volume, Velocity, Variety**,**Veracity** and **Value** .

These characteristics present distinct challenges and demands on data processing and analysis systems.

## Volume

**Volume** refers to the sheer **quantity of data** generated, stored, and processed, which far exceeds the capacity of traditional databases. Datasets can range from terabytes to petabytes (10^15 bytes) or even exabytes (10^18 bytes). 

Big Data often involves **automatically generated data**, which tends to accumulate rapidly. 

- **Sensor data** from industrial automation, manufacturing plants, or processing systems
- **Scanning devices** such as credit card and smart card readers
- **Measurement devices** like smart energy meters and environmental sensors
* The **Internet of Things (IoT)** has amplified this volume by connecting **billions of devices** that continuously generate data.
* **Social media platforms** like **Twitter**, **Facebook**, and **Instagram** contribute to massive volumes of data with each post, like, share, or message. 

**Impact on Processing and Analysis**: The sheer volume necessitates **parallel storage and processing** on very large clusters of machines, often thousands to tens of thousands of nodes, a scale most traditional parallel databases cannot handle. This has led to the development of systems like distributed file systems (e.g., Hadoop Distributed File System, HDFS) and key-value stores, which partition data across numerous nodes. 

- These systems enable linear scalability, allowing resources to be added to improve job latency and throughput linearly. 

## Velocity

Velocity refers to the speed at which data is created, transmitted, accumulated, ingested, and processed.

In today’s digital world, data flows at unprecedented speeds. 
- **Stock market systems** process **billions of transactions daily**
- **Social media** platforms see massive volumes of content created every second
- **Real-time applications** like fraud detection, personalized recommendations, and monitoring systems rely on **low-latency data ingestion and processing**
    
This dimension highlights the importance of high velocity data management  systems to being able to analyze and act on data in real time or near-real time.

Trending topics or breaking news detection is possible because of the **high-velocity streaming** and analysis of millions of posts.

## Variety

**Variety** refers to the **diverse types and formats** of data collected from multiple sources.

Traditional data sources were primarily structured, such as those found in: Financial systems, Insurance records, Retail transaction logs, Healthcare records
    
Data sources have expanded to include internet data (clickstream, social media posts, interactions), research data, location data, images, emails, supply chain data, sensor data, and videos. 

Modern Big Data systems must handle diverse data formats, including:
- **Structured data**: Tabular data from relational databases
- **Semi-structured data**: JSON, XML, logs, etc.
- **Unstructured data**: Text, audio, video, images, social media content, email, clickstreams, IoT data

The variety of data introduces complexity in processing, integration, and analysis, particularly for unstructured data, which forms the majority in many Big Data scenarios.

**Impact on Processing and Analysis**: The diversity of data formats means that traditional relational query languages like SQL are often not easily suited or efficient for all computations. 
**NoSQL systems** emerged to manage this variety, focusing on flexibility by not requiring a predefined schema and supporting semi-structured, self-describing data (e.g., MongoDB, CouchDB, HBase, Cassandra DB, Neo4j, Infinite Graph).


## Veracity

Veracity refers to the **credibility of the data source and the suitability of data for its target audience**, being closely related to trust.
**Veracity** concerns the **quality, accuracy, and trustworthiness** of the data. 

1. **Credibility of the source**: Is the data coming from a trusted and verified source?
    
2. **Suitability for target audience**: Is the data accurate, relevant, and reliable for the intended application?

Many sources generate data that is uncertain, incomplete, outdated or inaccurate, making its trustworthiness questionable. Since poor data quality can lead to **faulty analysis and wrong conclusions**. 

Veracity emphasizes the need to **evaluate and preprocess data** before using it in analytics, machine learning models, or business intelligence. Big Data systems must incorporate **data validation**, **cleansing**, and **quality assurance** mechanisms.

## Value

**Value** is often considered the **most critical dimension**. 

It refers to the **financial benefits and actionable insights** derived from processing and analyzing vast datasets. It reflects the **usefulness** of data in producing insights, making decisions, or creating business value.

Organizations invest substantially to gather data and build systems for data analytics because the financial benefits of making correct decisions are substantial, as are the costs of making wrong ones.

The pursuit of value drives the development and adoption of **data analytics, business intelligence, decision support, data mining, and machine learning techniques**.

